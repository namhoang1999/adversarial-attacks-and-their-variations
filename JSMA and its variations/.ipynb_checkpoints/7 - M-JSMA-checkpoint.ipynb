{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f729d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "%run pretrained-model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a0a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2085c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma_jacobian(model, X):\n",
    "    f = lambda image: model(image).to(device)\n",
    "    # 10 x 28 x 28 or 10 x 784\n",
    "    return jacobian(f, X).squeeze().view(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133b0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(jacobian, target, label, search_space):\n",
    "    \"\"\"Compute saliency map of an image\n",
    "\n",
    "    jacobian:     The jacobian matrix\n",
    "    target:       The target label\n",
    "    increasing:   Denote the use of incrementing or decrementing pixels method\n",
    "    search_space: The image search space \n",
    "    \n",
    "    return:       The saliency map\n",
    "    \"\"\" \n",
    "        \n",
    "    # The forward derivative of the target class\n",
    "    target_grad = jacobian[target]  \n",
    "    # The sum of forward derivative of all other classes\n",
    "    others_grad = all_sum = torch.sum(jacobian, dim=0) - target_grad  \n",
    "    \n",
    "    # Crossout pixels not in the search space\n",
    "    target_grad *= search_space \n",
    "    others_grad *= search_space\n",
    "\n",
    "    # Calculate sum of target forward derivative of any 2 features.\n",
    "    alpha = target_grad.reshape(-1, 1, 784) + target_grad.reshape(-1, 784, 1)  \n",
    "    # Calculate sum of other forward derivative of any 2 features.\n",
    "    beta = others_grad.reshape(-1, 1, 784) + others_grad.reshape(-1, 784, 1)\n",
    "\n",
    "    zero_mask = torch.ones(784, 784).fill_diagonal_(0).to(device)\n",
    "\n",
    "    # Form the actuall map, entries are either invalid (crossed out) or equal -alpha x beta\n",
    "    saliency_map = (-alpha * beta) * zero_mask\n",
    "    \n",
    "    # Calculate the sign map of alpha to compute Theta\n",
    "    sign_map = torch.sign(alpha) \n",
    "    if target == label:\n",
    "        sign_map = -sign_map\n",
    "    \n",
    "    return saliency_map, sign_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "246ef754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma(image, label, step_size, max_iters, model):\n",
    "    \"\"\"Perform JSMA attack on an image\n",
    "\n",
    "    image:     The input image X\n",
    "    label:     The image label\n",
    "    step_size: The perturbation size\n",
    "    max_iters: The maximum itrations of the attack\n",
    "    model:     The prediction model\n",
    "    \n",
    "    return:    The adversatial image X*\n",
    "    \"\"\" \n",
    "        \n",
    "    shape = image.shape\n",
    "    image = torch.flatten(image) # Flatten the image to 1D for easier modification \n",
    "    \n",
    "    search_domain = torch.lt(image, 2)\n",
    "    prediction = torch.argmax(model(image.reshape(shape))).item()\n",
    "\n",
    "    iter_ = 0\n",
    "    while (iter_ < max_iters) and (prediction == label) and (search_domain.sum() >= 2):\n",
    "        # Calculate Jacobian matrix \n",
    "        jacobian = jsma_jacobian(model, image.reshape(shape))\n",
    "        \n",
    "        map_, sign_map = saliency_map(jacobian, 0, label, search_domain)\n",
    "        for target in range(1,10):\n",
    "            # Calculate Jacobian matrix \n",
    "            jacobian = jsma_jacobian(model, image.reshape(shape))\n",
    "            temp_map, temp_sign_map = saliency_map(jacobian, target, label, search_domain)\n",
    "            \n",
    "            # Concatenate to the bigger map\n",
    "            map_ = torch.cat((map_, temp_map), 1)\n",
    "            sign_map = torch.cat((sign_map, temp_sign_map), 1)\n",
    "\n",
    "        # Get the two most salient pixels\n",
    "        val, idx = torch.max(map_.reshape(-1, 784 * 784 * 10), dim=1)\n",
    "        p1 = torch.div(idx, 784 * 10, rounding_mode='floor')\n",
    "        p2 = (idx % 784) % 784\n",
    "        \n",
    "        step = step_size * torch.flatten(sign_map)[idx.item()]\n",
    "        \n",
    "        # Modify pixels, and clip the image\n",
    "        image[p1] += step\n",
    "        image[p2] += step\n",
    "        image = torch.clamp(image, min=0.0, max=1.0)\n",
    "        \n",
    "        # Cross out modified pixels in the search space\n",
    "        search_domain[p1] = 0\n",
    "        search_domain[p2] = 0\n",
    "        \n",
    "        # Update the new label predicted by the model\n",
    "        prediction = torch.argmax(model(image.reshape(shape))).item()\n",
    "\n",
    "        iter_ += 1\n",
    "        \n",
    "    return image.reshape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02744a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12bce5fd940>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader_iter = iter(test_loader)\n",
    "input1 = next(test_loader_iter)\n",
    "image, label = input1[0].to(device), input1[1].item()\n",
    "\n",
    "adv_image = jsma(image     = image, \n",
    "                 label     = label,  \n",
    "                 step_size = 1, \n",
    "                 max_iters = 40,\n",
    "                 model     = model).reshape([1,1,28,28])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(f'{label} -> {torch.argmax(model(adv_image)).item()}')\n",
    "plt.imshow(adv_image.squeeze().cpu(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4847ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1 100.0\n",
      "2 / 2 100.0\n",
      "3 / 3 100.0\n",
      "4 / 4 100.0\n",
      "5 / 5 100.0\n",
      "6 / 7 85.71428571428571\n",
      "7 / 8 87.5\n",
      "8 / 9 88.88888888888889\n",
      "9 / 10 90.0\n",
      "10 / 11 90.9090909090909\n",
      "11 / 12 91.66666666666667\n",
      "12 / 13 92.3076923076923\n",
      "13 / 14 92.85714285714286\n",
      "14 / 15 93.33333333333333\n",
      "15 / 16 93.75\n",
      "16 / 17 94.11764705882354\n",
      "17 / 18 94.44444444444444\n",
      "18 / 19 94.73684210526316\n",
      "19 / 20 95.0\n",
      "20 / 21 95.23809523809524\n",
      "21 / 22 95.45454545454545\n",
      "22 / 23 95.65217391304348\n",
      "23 / 24 95.83333333333333\n",
      "24 / 25 96.0\n",
      "25 / 26 96.15384615384616\n",
      "26 / 27 96.29629629629629\n",
      "27 / 28 96.42857142857143\n",
      "28 / 29 96.55172413793103\n",
      "29 / 30 96.66666666666667\n",
      "30 / 32 93.75\n",
      "31 / 33 93.93939393939394\n",
      "32 / 34 94.11764705882354\n",
      "33 / 35 94.28571428571429\n",
      "34 / 36 94.44444444444444\n",
      "35 / 37 94.5945945945946\n",
      "36 / 38 94.73684210526316\n",
      "37 / 39 94.87179487179488\n",
      "38 / 40 95.0\n",
      "39 / 41 95.1219512195122\n",
      "40 / 42 95.23809523809524\n",
      "41 / 43 95.34883720930233\n",
      "42 / 44 95.45454545454545\n",
      "43 / 45 95.55555555555556\n",
      "44 / 46 95.65217391304348\n",
      "45 / 47 95.74468085106383\n",
      "46 / 48 95.83333333333333\n",
      "47 / 49 95.91836734693878\n",
      "48 / 50 96.0\n",
      "49 / 51 96.07843137254902\n",
      "50 / 52 96.15384615384616\n",
      "51 / 53 96.22641509433963\n",
      "52 / 54 96.29629629629629\n",
      "53 / 55 96.36363636363636\n",
      "54 / 56 96.42857142857143\n",
      "55 / 57 96.49122807017544\n",
      "56 / 58 96.55172413793103\n",
      "57 / 59 96.61016949152543\n",
      "58 / 60 96.66666666666667\n",
      "59 / 61 96.72131147540983\n",
      "60 / 62 96.7741935483871\n",
      "61 / 63 96.82539682539682\n",
      "62 / 64 96.875\n",
      "63 / 65 96.92307692307692\n",
      "64 / 66 96.96969696969697\n",
      "65 / 67 97.01492537313433\n",
      "66 / 68 97.05882352941177\n",
      "67 / 69 97.10144927536231\n",
      "68 / 71 95.77464788732394\n",
      "69 / 72 95.83333333333333\n",
      "70 / 73 95.89041095890411\n",
      "71 / 74 95.94594594594595\n",
      "72 / 75 96.0\n",
      "73 / 76 96.05263157894737\n",
      "74 / 77 96.1038961038961\n",
      "75 / 78 96.15384615384616\n",
      "76 / 79 96.20253164556962\n",
      "77 / 81 95.06172839506173\n",
      "78 / 82 95.1219512195122\n",
      "79 / 83 95.18072289156626\n",
      "80 / 84 95.23809523809524\n",
      "81 / 85 95.29411764705883\n",
      "82 / 86 95.34883720930233\n",
      "83 / 87 95.40229885057471\n",
      "84 / 88 95.45454545454545\n",
      "85 / 89 95.50561797752809\n",
      "86 / 90 95.55555555555556\n",
      "87 / 91 95.6043956043956\n",
      "88 / 92 95.65217391304348\n",
      "89 / 93 95.6989247311828\n",
      "90 / 94 95.74468085106383\n",
      "91 / 95 95.78947368421052\n",
      "92 / 96 95.83333333333333\n",
      "93 / 97 95.87628865979381\n",
      "94 / 98 95.91836734693878\n",
      "95 / 99 95.95959595959596\n",
      "96 / 100 96.0\n",
      "97 / 101 96.03960396039604\n",
      "98 / 102 96.07843137254902\n",
      "99 / 103 96.11650485436893\n",
      "100 / 104 96.15384615384616\n",
      "101 / 105 96.19047619047619\n",
      "102 / 106 96.22641509433963\n",
      "103 / 107 96.26168224299066\n",
      "104 / 108 96.29629629629629\n",
      "105 / 109 96.3302752293578\n",
      "106 / 110 96.36363636363636\n",
      "107 / 111 96.3963963963964\n",
      "108 / 112 96.42857142857143\n",
      "109 / 113 96.46017699115045\n",
      "110 / 114 96.49122807017544\n",
      "111 / 115 96.52173913043478\n",
      "112 / 116 96.55172413793103\n",
      "113 / 117 96.58119658119658\n",
      "114 / 118 96.61016949152543\n",
      "115 / 119 96.63865546218487\n",
      "116 / 120 96.66666666666667\n",
      "117 / 121 96.69421487603306\n",
      "118 / 122 96.72131147540983\n",
      "119 / 123 96.7479674796748\n",
      "120 / 124 96.7741935483871\n",
      "121 / 125 96.8\n",
      "122 / 126 96.82539682539682\n",
      "123 / 127 96.85039370078741\n",
      "124 / 128 96.875\n",
      "125 / 129 96.89922480620154\n",
      "126 / 130 96.92307692307692\n",
      "126 / 131 96.18320610687023\n",
      "127 / 132 96.21212121212122\n",
      "128 / 133 96.2406015037594\n",
      "129 / 134 96.26865671641791\n",
      "130 / 135 96.29629629629629\n",
      "131 / 136 96.32352941176471\n",
      "132 / 137 96.35036496350365\n",
      "133 / 138 96.3768115942029\n",
      "134 / 139 96.40287769784173\n",
      "135 / 140 96.42857142857143\n",
      "136 / 141 96.45390070921985\n",
      "137 / 142 96.47887323943662\n",
      "138 / 143 96.5034965034965\n",
      "139 / 144 96.52777777777777\n",
      "140 / 145 96.55172413793103\n",
      "141 / 146 96.57534246575342\n",
      "142 / 147 96.59863945578232\n",
      "143 / 148 96.62162162162163\n",
      "144 / 149 96.64429530201342\n",
      "145 / 150 96.66666666666667\n",
      "146 / 151 96.6887417218543\n",
      "147 / 152 96.71052631578948\n",
      "148 / 153 96.73202614379085\n",
      "149 / 154 96.75324675324676\n",
      "150 / 155 96.7741935483871\n",
      "151 / 156 96.7948717948718\n",
      "152 / 157 96.81528662420382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_prediction \u001b[38;5;241m!=\u001b[39m label:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m \u001b[43mjsma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m])\n\u001b[0;32m     17\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model(adv_image))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Correct if the prediction is the target label\u001b[39;00m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mjsma\u001b[1;34m(image, label, step_size, max_iters, model)\u001b[0m\n\u001b[0;32m     14\u001b[0m map_, sign_map \u001b[38;5;241m=\u001b[39m saliency_map(jacobian, \u001b[38;5;241m0\u001b[39m, label, search_domain)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Calculate Jacobian matrix \u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mjsma_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     temp_map, temp_sign_map \u001b[38;5;241m=\u001b[39m saliency_map(jacobian, target, label, search_domain)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Concatenate to the bigger map\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mjsma_jacobian\u001b[1;34m(model, X)\u001b[0m\n\u001b[0;32m      2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m image: model(image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 10 x 28 x 28 or 10 x 784\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[1;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[1;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:261\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    256\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    258\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    260\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[1;32m--> 261\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:68\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    image, label = next(test_loader_iter)\n",
    "    \n",
    "    initial_prediction = torch.argmax(model(image.to(device))).item()\n",
    "    # Don't bother attacking if the image is already misclassified\n",
    "    if initial_prediction != label:\n",
    "        continue\n",
    "\n",
    "    adv_image = jsma(image     = image.to(device), \n",
    "                     label     = label,  \n",
    "                     step_size = 1, \n",
    "                     max_iters = 40,\n",
    "                     model     = model).reshape([1,1,28,28])\n",
    "\n",
    "    prediction = torch.argmax(model(adv_image)).item()\n",
    "\n",
    "    # Correct if the prediction is the target label\n",
    "    if prediction != label:\n",
    "        correct += 1\n",
    "    \n",
    "    print(correct, '/', j+1, correct * 100 / (j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f898382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
