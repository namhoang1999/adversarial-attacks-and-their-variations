{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f729d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "%run pretrained-model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a0a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2085c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma_jacobian(model, X):\n",
    "    f = lambda image: model(image).to(device)\n",
    "    \n",
    "    # output shape 10 x 784\n",
    "    return jacobian(f, X).squeeze().reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "133b0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(jacobian, target, increasing, search_space, probability):\n",
    "    \"\"\"Compute saliency map of an image\n",
    "\n",
    "    jacobian:     The jacobian matrix\n",
    "    target:       The target label\n",
    "    increasing:   Denote the use of incrementing or decrementing pixels method\n",
    "    search_space: The image search space \n",
    "    probability:  The probability predicted by the model of all classes\n",
    "    \n",
    "    return:       The saliency map\n",
    "    \"\"\" \n",
    "    \n",
    "    # add the weighted term to all classes but the target class\n",
    "    probability[0, target] = 1\n",
    "    jacobian *= probability.reshape(10,1)\n",
    "    \n",
    "    # The forward derivative of the target class\n",
    "    target_grad = jacobian[target]  \n",
    "    # The sum of forward derivative of all other classes\n",
    "    others_grad = torch.sum(jacobian, dim=0) - target_grad  \n",
    "    \n",
    "    # Crossout pixels not in the search space\n",
    "    target_grad *= search_space \n",
    "    others_grad *= search_space\n",
    "\n",
    "    # Calculate sum of target forward derivative of any 2 features.\n",
    "    alpha = target_grad.reshape(-1, 1, 784) + target_grad.reshape(-1, 784, 1)  \n",
    "    # Calculate sum of other forward derivative of any 2 features.\n",
    "    beta = others_grad.reshape(-1, 1, 784) + others_grad.reshape(-1, 784, 1)\n",
    "\n",
    "    # Cross out entries that does not satisfy the condition (from formula 8 and 9)\n",
    "    condition1 = alpha > 0.0 if increasing else alpha < 0.0\n",
    "    condition2 = beta < 0.0 if increasing else beta > 0.0\n",
    "    zero_mask = torch.ones(784, 784).fill_diagonal_(0).to(device)\n",
    "\n",
    "    # Apply the condition to the saliency map\n",
    "    mask = (condition1 * condition2) * zero_mask\n",
    "    \n",
    "    # Form the actuall map, entries are either invalid (crossed out) or equal alpha x beta\n",
    "    saliency_map = alpha * torch.abs(beta) if increasing else torch.abs(alpha) * beta\n",
    "    saliency_map *= mask # cross out invalid entries\n",
    "    \n",
    "    # get the two most significant pixels\n",
    "    _, idx = torch.max(saliency_map.reshape(-1, 784 * 784), dim=1)\n",
    "    \n",
    "    p1 = torch.div(idx, 784, rounding_mode='floor')\n",
    "    p2 = idx % 784\n",
    "    \n",
    "    return p1.item(), p2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246ef754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wjsma(image, target, step_size, max_iters, model):\n",
    "    \"\"\"Perform Weighted JSMA attack on an image\n",
    "\n",
    "    image:     The input image X\n",
    "    target:    The target label\n",
    "    step_size: The perturbation size\n",
    "    max_iters: The maximum itrations of the attack\n",
    "    model:     The prediction model\n",
    "    \n",
    "    return:    The adversatial image X*\n",
    "    \"\"\" \n",
    "        \n",
    "    shape = image.shape\n",
    "    image = torch.flatten(image) # Flatten the image to 1D for easier modification \n",
    "    \n",
    "    increasing    = True if step_size > 0 else False\n",
    "    search_domain = image < 1 if increasing else image > 0\n",
    "    \n",
    "    # Label predicted by the model\n",
    "    probability = model(image.reshape(shape))\n",
    "    prediction = torch.argmax(probability).item()\n",
    "\n",
    "    iter_ = 0\n",
    "    while (iter_ < max_iters) and (prediction != target) and (search_domain.sum() != 0):\n",
    "        # Calculate Jacobian matrix \n",
    "        jacobian = jsma_jacobian(model, image.reshape(shape))\n",
    "        # Get the two most salient pixels\n",
    "        p1, p2 = saliency_map(jacobian, target, increasing, search_domain, probability)\n",
    "        \n",
    "        # Modify pixels, and clip the image\n",
    "        image[p1] += step_size\n",
    "        image[p2] += step_size\n",
    "        image = torch.clamp(image, min=0.0, max=1.0)\n",
    "        \n",
    "        # Cross out modified pixels in the search space\n",
    "        search_domain[p1] = 0\n",
    "        search_domain[p2] = 0\n",
    "        \n",
    "        # Update the new label predicted by the model\n",
    "        probability = model(image.reshape(shape))\n",
    "        prediction = torch.argmax(probability).item()\n",
    "\n",
    "        iter_ += 1\n",
    "\n",
    "    return image.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7331323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2180dac7e80>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader_iter = iter(test_loader)\n",
    "input1 = next(test_loader_iter)\n",
    "\n",
    "adv_image = wjsma(image     = input1[0].to(device), \n",
    "                  target    = 3,  \n",
    "                  step_size = 1, \n",
    "                  max_iters = 40,\n",
    "                  model     = model).reshape([1,1,28,28])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(torch.argmax(model(adv_image)))\n",
    "plt.imshow(adv_image.squeeze().cpu(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4847ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 / 9 100.0\n",
      "18 / 18 100.0\n",
      "27 / 27 100.0\n",
      "36 / 36 100.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;241m==\u001b[39m label\u001b[38;5;241m.\u001b[39mitem():\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m \u001b[43mwjsma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m])\n\u001b[0;32m     17\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model(adv_image))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Correct if the prediction is the target label\u001b[39;00m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mwjsma\u001b[1;34m(image, target, step_size, max_iters, model)\u001b[0m\n\u001b[0;32m     23\u001b[0m iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (iter_ \u001b[38;5;241m<\u001b[39m max_iters) \u001b[38;5;129;01mand\u001b[39;00m (prediction \u001b[38;5;241m!=\u001b[39m target) \u001b[38;5;129;01mand\u001b[39;00m (search_domain\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Calculate Jacobian matrix \u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mjsma_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Get the two most salient pixels\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     p1, p2 \u001b[38;5;241m=\u001b[39m saliency_map(jacobian, target, increasing, search_domain, probability)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mjsma_jacobian\u001b[1;34m(model, X)\u001b[0m\n\u001b[0;32m      2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m image: model(image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# output shape 10 x 784\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m784\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[1;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[1;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for j in range(100):\n",
    "    image, label = next(test_loader_iter)\n",
    "    \n",
    "    for target in range(10):\n",
    "        # Can't target the actual label can we?\n",
    "        if target == label.item():\n",
    "            continue\n",
    "        \n",
    "        adv_image = wjsma(image     = image.to(device), \n",
    "                          target    = target,  \n",
    "                          step_size = 1, \n",
    "                          max_iters = 40,\n",
    "                          model     = model).reshape([1,1,28,28])\n",
    "\n",
    "        prediction = torch.argmax(model(adv_image)).item()\n",
    "        \n",
    "        # Correct if the prediction is the target label\n",
    "        if prediction == target:\n",
    "            correct += 1\n",
    "    print(correct,'/', (j+1) * 9, correct * 100 / ((j+1) * 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39bd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
