{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f729d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "%run pretrained-model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a0a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2085c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma_jacobian(model, X):\n",
    "    f = lambda image: model(image).to(device)\n",
    "    \n",
    "    # output shape 10 x 784\n",
    "    return jacobian(f, X).squeeze().reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133b0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(jacobian, target, increasing, search_space, probability, image):\n",
    "    \"\"\"Compute saliency map of an image\n",
    "\n",
    "    jacobian:     The jacobian matrix\n",
    "    target:       The target label\n",
    "    increasing:   Denote the use of incrementing or decrementing pixels method\n",
    "    search_space: The image search space \n",
    "    probability:  The probability predicted by the model of all classes\n",
    "    image:        The input image X\n",
    "    \n",
    "    return:       The saliency map\n",
    "    \"\"\" \n",
    "    # add the Taylor term to all classes but the target class\n",
    "    probability[0, target] = 1\n",
    "    taylor = 1 - image if increasing else image\n",
    "    jacobian *= probability.reshape(10,1) * taylor \n",
    "    \n",
    "    # The forward derivative of the target class\n",
    "    target_grad = jacobian[target]  \n",
    "    # The sum of forward derivative of all other classes\n",
    "    others_grad = torch.sum(jacobian, dim=0) - target_grad  \n",
    "    \n",
    "    # Crossout pixels not in the search space\n",
    "    target_grad *= search_space \n",
    "    others_grad *= search_space\n",
    "\n",
    "    # Calculate sum of target forward derivative of any 2 features.\n",
    "    alpha = target_grad.reshape(-1, 1, 784) + target_grad.reshape(-1, 784, 1)  \n",
    "    # Calculate sum of other forward derivative of any 2 features.\n",
    "    beta = others_grad.reshape(-1, 1, 784) + others_grad.reshape(-1, 784, 1)\n",
    "\n",
    "    # Cross out entries that does not satisfy the condition (from formula 8 and 9)\n",
    "    condition1 = alpha < 0.0 if increasing else alpha > 0.0\n",
    "    condition2 = beta > 0.0 if increasing else beta < 0.0\n",
    "    zero_mask = torch.ones(784, 784).fill_diagonal_(0).to(device)\n",
    "\n",
    "    # Apply the condition to the saliency map\n",
    "    mask = (condition1 * condition2) * zero_mask\n",
    "    \n",
    "    # Form the actuall map, entries are either invalid (crossed out) or equal alpha x beta\n",
    "    saliency_map = torch.abs(alpha) * beta if increasing else alpha * torch.abs(beta)\n",
    "    saliency_map *= mask # cross out invalid entries\n",
    "    \n",
    "    # get the two most significant pixels\n",
    "    _, idx = torch.max(saliency_map.reshape(-1, 784 * 784), dim=1)\n",
    "    \n",
    "    p1 = torch.div(idx, 784, rounding_mode='floor')\n",
    "    p2 = idx % 784\n",
    "    \n",
    "    return p1.item(), p2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246ef754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma(image, label, step_size, max_iters, model):\n",
    "    \"\"\"Perform JSMA attack on an image\n",
    "\n",
    "    image:     The input image X\n",
    "    label:     The image label\n",
    "    step_size: The perturbation size\n",
    "    max_iters: The maximum itrations of the attack\n",
    "    model:     The prediction model\n",
    "    \n",
    "    return:    The adversatial image X*\n",
    "    \"\"\" \n",
    "        \n",
    "    shape = image.shape\n",
    "    image = torch.flatten(image) # Flatten the image to 1D for easier modification \n",
    "    \n",
    "    increasing    = True if step_size > 0 else False\n",
    "    search_domain = image < 1 if increasing else image > 0\n",
    "    \n",
    "    # Label predicted by the model\n",
    "    probability = model(image.reshape(shape))\n",
    "    prediction = torch.argmax(probability).item()\n",
    "\n",
    "    iter_ = 0\n",
    "    while (iter_ < max_iters) and (prediction == label) and (search_domain.sum() != 0):\n",
    "        # Calculate Jacobian matrix \n",
    "        jacobian = jsma_jacobian(model, image.reshape(shape))\n",
    "        # Get the two most salient pixels\n",
    "        p1, p2 = saliency_map(jacobian, label, increasing, search_domain, probability, image)\n",
    "        \n",
    "        # Modify pixels, and clip the image\n",
    "        image[p1] += step_size\n",
    "        image[p2] += step_size\n",
    "        image = torch.clamp(image, min=0.0, max=1.0)\n",
    "        \n",
    "        # Cross out modified pixels in the search space\n",
    "        search_domain[p1] = 0\n",
    "        search_domain[p2] = 0\n",
    "        \n",
    "        # Update the new label predicted by the model\n",
    "        probability = model(image.reshape(shape))\n",
    "        prediction = torch.argmax(probability).item()\n",
    "\n",
    "        iter_ += 1\n",
    "\n",
    "    return image.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7331323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27c051c1f10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader_iter = iter(test_loader)\n",
    "input1 = next(test_loader_iter)\n",
    "print(device)\n",
    "adv_image = jsma(image     = input1[0].to(device), \n",
    "                 label     = input1[1].item(),  \n",
    "                 step_size = 1, \n",
    "                 max_iters = 40,\n",
    "                 model     = model).reshape([1,1,28,28])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(torch.argmax(model(adv_image)))\n",
    "plt.imshow(adv_image.squeeze().cpu(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4847ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1 100.0\n",
      "2 / 2 100.0\n",
      "3 / 3 100.0\n",
      "4 / 4 100.0\n",
      "5 / 5 100.0\n",
      "6 / 6 100.0\n",
      "7 / 7 100.0\n",
      "8 / 8 100.0\n",
      "9 / 9 100.0\n",
      "10 / 10 100.0\n",
      "11 / 11 100.0\n",
      "12 / 12 100.0\n",
      "13 / 13 100.0\n",
      "14 / 14 100.0\n",
      "15 / 15 100.0\n",
      "16 / 16 100.0\n",
      "17 / 17 100.0\n",
      "18 / 18 100.0\n",
      "19 / 19 100.0\n",
      "20 / 20 100.0\n",
      "21 / 22 95.45454545454545\n",
      "22 / 23 95.65217391304348\n",
      "23 / 24 95.83333333333333\n",
      "24 / 25 96.0\n",
      "25 / 26 96.15384615384616\n",
      "26 / 27 96.29629629629629\n",
      "27 / 28 96.42857142857143\n",
      "28 / 29 96.55172413793103\n",
      "29 / 30 96.66666666666667\n",
      "30 / 31 96.7741935483871\n",
      "31 / 32 96.875\n",
      "32 / 33 96.96969696969697\n",
      "33 / 34 97.05882352941177\n",
      "34 / 35 97.14285714285714\n",
      "35 / 36 97.22222222222223\n",
      "36 / 37 97.29729729729729\n",
      "37 / 38 97.36842105263158\n",
      "38 / 39 97.43589743589743\n",
      "39 / 40 97.5\n",
      "40 / 41 97.5609756097561\n",
      "41 / 42 97.61904761904762\n",
      "42 / 43 97.67441860465117\n",
      "43 / 44 97.72727272727273\n",
      "44 / 45 97.77777777777777\n",
      "45 / 46 97.82608695652173\n",
      "46 / 47 97.87234042553192\n",
      "47 / 48 97.91666666666667\n",
      "48 / 49 97.95918367346938\n",
      "49 / 50 98.0\n",
      "50 / 51 98.03921568627452\n",
      "51 / 52 98.07692307692308\n",
      "52 / 53 98.11320754716981\n",
      "53 / 54 98.14814814814815\n",
      "54 / 55 98.18181818181819\n",
      "55 / 56 98.21428571428571\n",
      "56 / 57 98.24561403508773\n",
      "57 / 58 98.27586206896552\n",
      "58 / 59 98.30508474576271\n",
      "59 / 60 98.33333333333333\n",
      "60 / 61 98.36065573770492\n",
      "61 / 62 98.38709677419355\n",
      "62 / 63 98.41269841269842\n",
      "63 / 64 98.4375\n",
      "64 / 65 98.46153846153847\n",
      "65 / 66 98.48484848484848\n",
      "66 / 67 98.50746268656717\n",
      "67 / 68 98.52941176470588\n",
      "68 / 69 98.55072463768116\n",
      "69 / 70 98.57142857142857\n",
      "70 / 71 98.59154929577464\n",
      "71 / 72 98.61111111111111\n",
      "72 / 73 98.63013698630137\n",
      "73 / 74 98.64864864864865\n",
      "74 / 75 98.66666666666667\n",
      "75 / 76 98.6842105263158\n",
      "76 / 77 98.7012987012987\n",
      "77 / 78 98.71794871794872\n",
      "78 / 79 98.73417721518987\n",
      "79 / 80 98.75\n",
      "80 / 81 98.76543209876543\n",
      "81 / 82 98.78048780487805\n",
      "82 / 83 98.79518072289157\n",
      "83 / 84 98.80952380952381\n",
      "84 / 85 98.82352941176471\n",
      "85 / 86 98.83720930232558\n",
      "86 / 87 98.85057471264368\n",
      "87 / 88 98.86363636363636\n",
      "88 / 89 98.87640449438203\n",
      "89 / 90 98.88888888888889\n",
      "90 / 91 98.9010989010989\n",
      "91 / 92 98.91304347826087\n",
      "92 / 93 98.9247311827957\n",
      "93 / 94 98.93617021276596\n",
      "94 / 95 98.94736842105263\n",
      "95 / 96 98.95833333333333\n",
      "96 / 97 98.96907216494846\n",
      "97 / 98 98.9795918367347\n",
      "98 / 99 98.98989898989899\n",
      "99 / 100 99.0\n",
      "100 / 101 99.00990099009901\n",
      "101 / 102 99.01960784313725\n",
      "102 / 103 99.02912621359224\n",
      "103 / 104 99.03846153846153\n",
      "104 / 105 99.04761904761905\n",
      "105 / 106 99.05660377358491\n",
      "106 / 107 99.06542056074767\n",
      "107 / 108 99.07407407407408\n",
      "108 / 109 99.08256880733946\n",
      "109 / 110 99.0909090909091\n",
      "110 / 111 99.09909909909909\n",
      "111 / 112 99.10714285714286\n",
      "112 / 113 99.11504424778761\n",
      "113 / 114 99.12280701754386\n",
      "114 / 115 99.1304347826087\n",
      "115 / 116 99.13793103448276\n",
      "116 / 117 99.14529914529915\n",
      "117 / 118 99.15254237288136\n",
      "118 / 119 99.15966386554622\n",
      "119 / 120 99.16666666666667\n",
      "120 / 121 99.17355371900827\n",
      "121 / 122 99.18032786885246\n",
      "122 / 123 99.1869918699187\n",
      "123 / 124 99.19354838709677\n",
      "124 / 125 99.2\n",
      "125 / 126 99.2063492063492\n",
      "126 / 127 99.21259842519684\n",
      "127 / 128 99.21875\n",
      "128 / 129 99.2248062015504\n",
      "129 / 130 99.23076923076923\n",
      "130 / 131 99.23664122137404\n",
      "131 / 132 99.24242424242425\n",
      "132 / 133 99.24812030075188\n",
      "133 / 134 99.25373134328358\n",
      "134 / 135 99.25925925925925\n",
      "135 / 136 99.26470588235294\n",
      "136 / 137 99.27007299270073\n",
      "137 / 138 99.27536231884058\n",
      "138 / 139 99.28057553956835\n",
      "139 / 140 99.28571428571429\n",
      "140 / 141 99.29078014184397\n",
      "141 / 142 99.29577464788733\n",
      "142 / 143 99.3006993006993\n",
      "143 / 144 99.30555555555556\n",
      "144 / 145 99.3103448275862\n",
      "145 / 146 99.31506849315069\n",
      "146 / 147 99.31972789115646\n",
      "147 / 148 99.32432432432432\n",
      "148 / 149 99.32885906040268\n",
      "149 / 150 99.33333333333333\n",
      "150 / 151 99.33774834437087\n",
      "151 / 152 99.34210526315789\n",
      "152 / 153 99.34640522875817\n",
      "153 / 154 99.35064935064935\n",
      "154 / 155 99.35483870967742\n",
      "155 / 156 99.35897435897436\n",
      "156 / 157 99.36305732484077\n",
      "157 / 158 99.36708860759494\n",
      "158 / 159 99.37106918238993\n",
      "159 / 160 99.375\n",
      "160 / 161 99.37888198757764\n",
      "161 / 162 99.38271604938272\n",
      "162 / 163 99.38650306748467\n",
      "163 / 164 99.39024390243902\n",
      "164 / 165 99.39393939393939\n",
      "165 / 166 99.39759036144578\n",
      "166 / 167 99.40119760479043\n",
      "167 / 168 99.4047619047619\n",
      "168 / 169 99.40828402366864\n",
      "169 / 170 99.41176470588235\n",
      "170 / 171 99.41520467836257\n",
      "171 / 172 99.4186046511628\n",
      "172 / 173 99.42196531791907\n",
      "173 / 174 99.42528735632185\n",
      "174 / 175 99.42857142857143\n",
      "175 / 176 99.43181818181819\n",
      "176 / 177 99.43502824858757\n",
      "177 / 179 98.88268156424581\n",
      "178 / 180 98.88888888888889\n",
      "179 / 181 98.89502762430939\n",
      "180 / 182 98.9010989010989\n",
      "181 / 183 98.90710382513662\n",
      "182 / 184 98.91304347826087\n",
      "183 / 185 98.91891891891892\n",
      "184 / 186 98.9247311827957\n",
      "185 / 187 98.93048128342247\n",
      "186 / 188 98.93617021276596\n",
      "187 / 189 98.94179894179894\n",
      "188 / 190 98.94736842105263\n",
      "189 / 191 98.95287958115183\n",
      "190 / 193 98.44559585492227\n",
      "191 / 194 98.45360824742268\n",
      "192 / 195 98.46153846153847\n",
      "193 / 196 98.46938775510205\n",
      "194 / 197 98.4771573604061\n",
      "195 / 198 98.48484848484848\n",
      "196 / 199 98.49246231155779\n",
      "197 / 200 98.5\n",
      "198 / 201 98.50746268656717\n",
      "199 / 202 98.51485148514851\n",
      "200 / 203 98.52216748768473\n",
      "201 / 204 98.52941176470588\n",
      "202 / 205 98.53658536585365\n",
      "203 / 206 98.54368932038835\n",
      "204 / 207 98.55072463768116\n",
      "205 / 208 98.5576923076923\n",
      "206 / 209 98.56459330143541\n",
      "207 / 210 98.57142857142857\n",
      "208 / 211 98.5781990521327\n",
      "209 / 212 98.58490566037736\n",
      "210 / 213 98.59154929577464\n",
      "211 / 214 98.59813084112149\n",
      "212 / 215 98.6046511627907\n",
      "213 / 216 98.61111111111111\n",
      "214 / 217 98.61751152073732\n",
      "215 / 218 98.62385321100918\n",
      "216 / 219 98.63013698630137\n",
      "217 / 220 98.63636363636364\n",
      "218 / 221 98.64253393665159\n",
      "219 / 222 98.64864864864865\n",
      "220 / 223 98.65470852017937\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_prediction \u001b[38;5;241m!=\u001b[39m label:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m \u001b[43mjsma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m])\n\u001b[0;32m     17\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model(adv_image))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Correct if the prediction is the target label\u001b[39;00m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mjsma\u001b[1;34m(image, label, step_size, max_iters, model)\u001b[0m\n\u001b[0;32m     23\u001b[0m iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (iter_ \u001b[38;5;241m<\u001b[39m max_iters) \u001b[38;5;129;01mand\u001b[39;00m (prediction \u001b[38;5;241m==\u001b[39m label) \u001b[38;5;129;01mand\u001b[39;00m (search_domain\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Calculate Jacobian matrix \u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mjsma_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Get the two most salient pixels\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     p1, p2 \u001b[38;5;241m=\u001b[39m saliency_map(jacobian, label, increasing, search_domain, probability, image)\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mjsma_jacobian\u001b[1;34m(model, X)\u001b[0m\n\u001b[0;32m      2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m image: model(image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# output shape 10 x 784\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[1;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[1;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    image, label = next(test_loader_iter)\n",
    "    \n",
    "    initial_prediction = torch.argmax(model(image.to(device))).item()\n",
    "    # Don't bother attacking if the image is already misclassified\n",
    "    if initial_prediction != label:\n",
    "        continue\n",
    "\n",
    "    adv_image = jsma(image     = image.to(device), \n",
    "                     label     = label,  \n",
    "                     step_size = 1, \n",
    "                     max_iters = 40,\n",
    "                     model     = model).reshape([1,1,28,28])\n",
    "\n",
    "    prediction = torch.argmax(model(adv_image)).item()\n",
    "\n",
    "    # Correct if the prediction is the target label\n",
    "    if prediction != label:\n",
    "        correct += 1\n",
    "    \n",
    "    print(correct, '/', j+1, correct * 100 / (j+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
