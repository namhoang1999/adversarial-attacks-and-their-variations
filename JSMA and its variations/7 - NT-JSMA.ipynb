{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f729d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n"
     ]
    }
   ],
   "source": [
    "%run pretrained-model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a0a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd.functional import jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2085c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma_jacobian(model, X):\n",
    "    f = lambda image: model(image).to(device)\n",
    "    \n",
    "    # output shape 10 x 784\n",
    "    return jacobian(f, X).squeeze().reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133b0b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(jacobian, target, increasing, search_space):\n",
    "    \"\"\"Compute saliency map of an image\n",
    "\n",
    "    jacobian:     The jacobian matrix\n",
    "    target:       The target label\n",
    "    increasing:   Denote the use of incrementing or decrementing pixels method\n",
    "    search_space: The image search space \n",
    "    \n",
    "    return:       The saliency map\n",
    "    \"\"\" \n",
    "\n",
    "    # The forward derivative of the target class\n",
    "    target_grad = jacobian[target]  \n",
    "    # The sum of forward derivative of all other classes\n",
    "    others_grad = all_sum = torch.sum(jacobian, dim=0) - target_grad  \n",
    "    \n",
    "    # Crossout pixels not in the search space\n",
    "    target_grad *= search_space \n",
    "    others_grad *= search_space\n",
    "\n",
    "    # Calculate sum of target forward derivative of any 2 features.\n",
    "    alpha = target_grad.reshape(-1, 1, 784) + target_grad.reshape(-1, 784, 1)  \n",
    "    # Calculate sum of other forward derivative of any 2 features.\n",
    "    beta = others_grad.reshape(-1, 1, 784) + others_grad.reshape(-1, 784, 1)\n",
    "\n",
    "    # Cross out entries that does not satisfy the condition (from formula 8 and 9)\n",
    "    condition1 = alpha < 0.0 if increasing else alpha > 0.0\n",
    "    condition2 = beta > 0.0 if increasing else beta < 0.0\n",
    "    zero_mask = torch.ones(784, 784).fill_diagonal_(0).to(device)\n",
    "\n",
    "    # Apply the condition to the saliency map\n",
    "    mask = (condition1 * condition2) * zero_mask\n",
    "    \n",
    "    # Form the actuall map, entries are either invalid (crossed out) or equal alpha x beta\n",
    "    saliency_map = (torch.abs(alpha) * beta) if increasing else (alpha * torch.abs(beta)) \n",
    "    saliency_map *= mask # cross out invalid entries\n",
    "    \n",
    "    # get the two most significant pixels\n",
    "    _, idx = torch.max(saliency_map.reshape(-1, 784 * 784), dim=1)\n",
    "    \n",
    "    p1 = torch.div(idx, 784, rounding_mode='floor')\n",
    "    p2 = idx % 784\n",
    "    \n",
    "    return p1.item(), p2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "246ef754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsma(image, label, step_size, max_iters, model):\n",
    "    \"\"\"Perform JSMA attack on an image\n",
    "\n",
    "    image:     The input image X\n",
    "    label:     The image label\n",
    "    step_size: The perturbation size\n",
    "    max_iters: The maximum itrations of the attack\n",
    "    model:     The prediction model\n",
    "    \n",
    "    return:    The adversatial image X*\n",
    "    \"\"\" \n",
    "        \n",
    "    shape = image.shape\n",
    "    image = torch.flatten(image) # Flatten the image to 1D for easier modification \n",
    "    \n",
    "    increasing    = True if step_size > 0 else False\n",
    "    search_domain = image < 1 if increasing else image > 0\n",
    "    \n",
    "    # Label predicted by the model\n",
    "    prediction = torch.argmax(model(image.reshape(shape))).item()\n",
    "\n",
    "    iter_ = 0\n",
    "    while (iter_ < max_iters) and (prediction == label) and (search_domain.sum() != 0):\n",
    "        # Calculate Jacobian matrix \n",
    "        jacobian = jsma_jacobian(model, image.reshape(shape))\n",
    "        # Get the two most salient pixels\n",
    "        p1, p2 = saliency_map(jacobian, label, increasing, search_domain)\n",
    "        \n",
    "        # Modify pixels, and clip the image\n",
    "        image[p1] += step_size\n",
    "        image[p2] += step_size\n",
    "        image = torch.clamp(image, min=0.0, max=1.0)\n",
    "        \n",
    "        # Cross out modified pixels in the search space\n",
    "        search_domain[p1] = 0\n",
    "        search_domain[p2] = 0\n",
    "        \n",
    "        # Update the new label predicted by the model\n",
    "        prediction = torch.argmax(model(image.reshape(shape))).item()\n",
    "\n",
    "        iter_ += 1\n",
    "\n",
    "    return image.reshape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7331323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23d045f6e50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_loader_iter = iter(test_loader)\n",
    "input1 = next(test_loader_iter)\n",
    "print(device)\n",
    "adv_image = jsma(image     = input1[0].to(device), \n",
    "                 label     = input1[1].item(),  \n",
    "                 step_size = 1, \n",
    "                 max_iters = 40,\n",
    "                 model     = model).reshape([1,1,28,28])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.title(torch.argmax(model(adv_image)))\n",
    "plt.imshow(adv_image.squeeze().cpu(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4847ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1 100.0\n",
      "2 / 2 100.0\n",
      "3 / 3 100.0\n",
      "4 / 4 100.0\n",
      "5 / 5 100.0\n",
      "6 / 6 100.0\n",
      "7 / 7 100.0\n",
      "8 / 8 100.0\n",
      "9 / 9 100.0\n",
      "10 / 10 100.0\n",
      "11 / 11 100.0\n",
      "12 / 12 100.0\n",
      "13 / 13 100.0\n",
      "14 / 14 100.0\n",
      "15 / 15 100.0\n",
      "16 / 16 100.0\n",
      "17 / 17 100.0\n",
      "18 / 18 100.0\n",
      "19 / 19 100.0\n",
      "20 / 20 100.0\n",
      "21 / 21 100.0\n",
      "22 / 22 100.0\n",
      "23 / 23 100.0\n",
      "24 / 24 100.0\n",
      "25 / 25 100.0\n",
      "26 / 26 100.0\n",
      "27 / 27 100.0\n",
      "28 / 28 100.0\n",
      "29 / 29 100.0\n",
      "30 / 30 100.0\n",
      "31 / 31 100.0\n",
      "32 / 32 100.0\n",
      "33 / 33 100.0\n",
      "34 / 34 100.0\n",
      "35 / 35 100.0\n",
      "36 / 36 100.0\n",
      "37 / 37 100.0\n",
      "38 / 38 100.0\n",
      "39 / 39 100.0\n",
      "40 / 40 100.0\n",
      "41 / 41 100.0\n",
      "42 / 42 100.0\n",
      "43 / 43 100.0\n",
      "44 / 44 100.0\n",
      "45 / 45 100.0\n",
      "46 / 46 100.0\n",
      "47 / 47 100.0\n",
      "48 / 48 100.0\n",
      "49 / 49 100.0\n",
      "50 / 50 100.0\n",
      "51 / 51 100.0\n",
      "52 / 52 100.0\n",
      "53 / 53 100.0\n",
      "54 / 54 100.0\n",
      "55 / 55 100.0\n",
      "56 / 56 100.0\n",
      "57 / 57 100.0\n",
      "58 / 58 100.0\n",
      "59 / 59 100.0\n",
      "60 / 61 98.36065573770492\n",
      "61 / 62 98.38709677419355\n",
      "62 / 63 98.41269841269842\n",
      "63 / 64 98.4375\n",
      "64 / 65 98.46153846153847\n",
      "65 / 66 98.48484848484848\n",
      "66 / 67 98.50746268656717\n",
      "67 / 68 98.52941176470588\n",
      "68 / 69 98.55072463768116\n",
      "69 / 71 97.1830985915493\n",
      "70 / 72 97.22222222222223\n",
      "71 / 73 97.26027397260275\n",
      "72 / 74 97.29729729729729\n",
      "73 / 75 97.33333333333333\n",
      "74 / 76 97.36842105263158\n",
      "75 / 77 97.40259740259741\n",
      "76 / 78 97.43589743589743\n",
      "77 / 79 97.46835443037975\n",
      "78 / 80 97.5\n",
      "79 / 81 97.53086419753086\n",
      "80 / 82 97.5609756097561\n",
      "81 / 83 97.59036144578313\n",
      "82 / 84 97.61904761904762\n",
      "83 / 85 97.6470588235294\n",
      "84 / 86 97.67441860465117\n",
      "85 / 87 97.70114942528735\n",
      "86 / 88 97.72727272727273\n",
      "87 / 89 97.75280898876404\n",
      "88 / 90 97.77777777777777\n",
      "89 / 91 97.8021978021978\n",
      "90 / 92 97.82608695652173\n",
      "91 / 93 97.84946236559139\n",
      "92 / 94 97.87234042553192\n",
      "93 / 95 97.89473684210526\n",
      "94 / 96 97.91666666666667\n",
      "95 / 97 97.9381443298969\n",
      "96 / 98 97.95918367346938\n",
      "97 / 99 97.97979797979798\n",
      "98 / 100 98.0\n",
      "99 / 101 98.01980198019803\n",
      "100 / 102 98.03921568627452\n",
      "101 / 103 98.05825242718447\n",
      "102 / 104 98.07692307692308\n",
      "103 / 105 98.0952380952381\n",
      "104 / 106 98.11320754716981\n",
      "105 / 107 98.13084112149532\n",
      "106 / 108 98.14814814814815\n",
      "107 / 109 98.1651376146789\n",
      "108 / 111 97.29729729729729\n",
      "109 / 112 97.32142857142857\n",
      "110 / 113 97.34513274336283\n",
      "111 / 114 97.36842105263158\n",
      "112 / 115 97.3913043478261\n",
      "113 / 116 97.41379310344827\n",
      "114 / 117 97.43589743589743\n",
      "115 / 118 97.45762711864407\n",
      "116 / 119 97.47899159663865\n",
      "117 / 120 97.5\n",
      "118 / 121 97.52066115702479\n",
      "119 / 122 97.54098360655738\n",
      "120 / 123 97.5609756097561\n",
      "121 / 124 97.58064516129032\n",
      "122 / 125 97.6\n",
      "123 / 126 97.61904761904762\n",
      "124 / 127 97.63779527559055\n",
      "125 / 128 97.65625\n",
      "126 / 129 97.67441860465117\n",
      "127 / 130 97.6923076923077\n",
      "128 / 133 96.2406015037594\n",
      "129 / 134 96.26865671641791\n",
      "130 / 135 96.29629629629629\n",
      "131 / 136 96.32352941176471\n",
      "132 / 137 96.35036496350365\n",
      "133 / 138 96.3768115942029\n",
      "134 / 139 96.40287769784173\n",
      "135 / 140 96.42857142857143\n",
      "136 / 141 96.45390070921985\n",
      "137 / 142 96.47887323943662\n",
      "138 / 143 96.5034965034965\n",
      "139 / 144 96.52777777777777\n",
      "140 / 145 96.55172413793103\n",
      "141 / 146 96.57534246575342\n",
      "142 / 147 96.59863945578232\n",
      "143 / 148 96.62162162162163\n",
      "144 / 149 96.64429530201342\n",
      "145 / 150 96.66666666666667\n",
      "146 / 151 96.6887417218543\n",
      "147 / 152 96.71052631578948\n",
      "148 / 153 96.73202614379085\n",
      "149 / 154 96.75324675324676\n",
      "150 / 155 96.7741935483871\n",
      "151 / 156 96.7948717948718\n",
      "152 / 157 96.81528662420382\n",
      "153 / 158 96.83544303797468\n",
      "154 / 159 96.85534591194968\n",
      "155 / 160 96.875\n",
      "156 / 161 96.8944099378882\n",
      "157 / 162 96.91358024691358\n",
      "158 / 163 96.93251533742331\n",
      "159 / 164 96.95121951219512\n",
      "160 / 165 96.96969696969697\n",
      "161 / 166 96.98795180722891\n",
      "162 / 167 97.0059880239521\n",
      "163 / 168 97.02380952380952\n",
      "164 / 169 97.0414201183432\n",
      "165 / 170 97.05882352941177\n",
      "166 / 171 97.07602339181287\n",
      "167 / 172 97.09302325581395\n",
      "168 / 173 97.10982658959537\n",
      "169 / 174 97.1264367816092\n",
      "170 / 175 97.14285714285714\n",
      "171 / 176 97.1590909090909\n",
      "172 / 177 97.17514124293785\n",
      "173 / 178 97.19101123595506\n",
      "174 / 179 97.20670391061452\n",
      "175 / 180 97.22222222222223\n",
      "176 / 181 97.23756906077348\n",
      "177 / 182 97.25274725274726\n",
      "178 / 183 97.26775956284153\n",
      "179 / 184 97.28260869565217\n",
      "180 / 185 97.29729729729729\n",
      "181 / 186 97.31182795698925\n",
      "182 / 187 97.32620320855615\n",
      "183 / 188 97.34042553191489\n",
      "184 / 189 97.35449735449735\n",
      "185 / 190 97.36842105263158\n",
      "186 / 191 97.38219895287958\n",
      "187 / 192 97.39583333333333\n",
      "188 / 193 97.40932642487047\n",
      "189 / 194 97.42268041237114\n",
      "190 / 195 97.43589743589743\n",
      "191 / 196 97.44897959183673\n",
      "192 / 197 97.46192893401015\n",
      "193 / 198 97.47474747474747\n",
      "194 / 199 97.48743718592965\n",
      "195 / 200 97.5\n",
      "196 / 201 97.51243781094527\n",
      "197 / 202 97.52475247524752\n",
      "198 / 203 97.53694581280789\n",
      "199 / 204 97.54901960784314\n",
      "200 / 205 97.5609756097561\n",
      "201 / 206 97.57281553398059\n",
      "202 / 207 97.58454106280193\n",
      "203 / 208 97.59615384615384\n",
      "204 / 209 97.60765550239235\n",
      "205 / 210 97.61904761904762\n",
      "206 / 211 97.6303317535545\n",
      "207 / 212 97.64150943396227\n",
      "208 / 213 97.65258215962442\n",
      "209 / 214 97.66355140186916\n",
      "210 / 215 97.67441860465117\n",
      "211 / 216 97.68518518518519\n",
      "212 / 217 97.6958525345622\n",
      "213 / 218 97.70642201834862\n",
      "214 / 219 97.71689497716895\n",
      "215 / 220 97.72727272727273\n",
      "216 / 221 97.73755656108597\n",
      "217 / 222 97.74774774774775\n",
      "218 / 223 97.75784753363229\n",
      "219 / 224 97.76785714285714\n",
      "220 / 225 97.77777777777777\n",
      "221 / 226 97.78761061946902\n",
      "222 / 227 97.79735682819383\n",
      "223 / 228 97.80701754385964\n",
      "224 / 229 97.81659388646288\n",
      "225 / 230 97.82608695652173\n",
      "226 / 231 97.83549783549783\n",
      "227 / 232 97.84482758620689\n",
      "228 / 233 97.85407725321889\n",
      "229 / 234 97.86324786324786\n",
      "230 / 235 97.87234042553192\n",
      "231 / 236 97.88135593220339\n",
      "232 / 237 97.8902953586498\n",
      "233 / 238 97.89915966386555\n",
      "234 / 239 97.90794979079497\n",
      "235 / 240 97.91666666666667\n",
      "236 / 241 97.9253112033195\n",
      "237 / 242 97.93388429752066\n",
      "238 / 243 97.94238683127573\n",
      "239 / 244 97.95081967213115\n",
      "240 / 245 97.95918367346938\n",
      "241 / 246 97.96747967479675\n",
      "242 / 247 97.97570850202429\n",
      "243 / 248 97.98387096774194\n",
      "244 / 249 97.99196787148594\n",
      "245 / 250 98.0\n",
      "246 / 251 98.00796812749005\n",
      "247 / 252 98.01587301587301\n",
      "248 / 253 98.02371541501977\n",
      "249 / 254 98.03149606299213\n",
      "250 / 255 98.03921568627452\n",
      "251 / 256 98.046875\n",
      "252 / 257 98.05447470817121\n",
      "253 / 258 98.06201550387597\n",
      "254 / 259 98.06949806949807\n",
      "255 / 260 98.07692307692308\n",
      "256 / 261 98.08429118773947\n",
      "257 / 262 98.09160305343511\n",
      "258 / 263 98.09885931558935\n",
      "259 / 264 98.10606060606061\n",
      "260 / 265 98.11320754716981\n",
      "261 / 266 98.1203007518797\n",
      "262 / 267 98.12734082397004\n",
      "263 / 268 98.13432835820896\n",
      "264 / 269 98.14126394052045\n",
      "265 / 270 98.14814814814815\n",
      "266 / 271 98.1549815498155\n",
      "267 / 272 98.16176470588235\n",
      "268 / 273 98.16849816849818\n",
      "269 / 274 98.17518248175182\n",
      "270 / 275 98.18181818181819\n",
      "271 / 276 98.18840579710145\n",
      "272 / 277 98.19494584837545\n",
      "273 / 279 97.84946236559139\n",
      "274 / 280 97.85714285714286\n",
      "275 / 281 97.86476868327402\n",
      "276 / 282 97.87234042553192\n",
      "277 / 283 97.87985865724382\n",
      "278 / 284 97.88732394366197\n",
      "279 / 285 97.89473684210526\n",
      "280 / 286 97.9020979020979\n",
      "281 / 287 97.90940766550523\n",
      "282 / 288 97.91666666666667\n",
      "283 / 289 97.92387543252595\n",
      "284 / 291 97.59450171821305\n",
      "285 / 292 97.6027397260274\n",
      "286 / 293 97.61092150170649\n",
      "287 / 294 97.61904761904762\n",
      "288 / 295 97.62711864406779\n",
      "289 / 296 97.63513513513513\n",
      "290 / 297 97.64309764309765\n",
      "291 / 298 97.6510067114094\n",
      "292 / 299 97.65886287625418\n",
      "293 / 300 97.66666666666667\n",
      "294 / 301 97.67441860465117\n",
      "295 / 302 97.68211920529801\n",
      "296 / 303 97.6897689768977\n",
      "297 / 304 97.69736842105263\n",
      "298 / 305 97.70491803278688\n",
      "299 / 306 97.7124183006536\n",
      "300 / 307 97.71986970684038\n",
      "301 / 308 97.72727272727273\n",
      "302 / 309 97.73462783171522\n",
      "303 / 310 97.74193548387096\n",
      "304 / 311 97.7491961414791\n",
      "305 / 312 97.75641025641026\n",
      "306 / 313 97.76357827476038\n",
      "307 / 314 97.77070063694268\n",
      "308 / 315 97.77777777777777\n",
      "309 / 316 97.78481012658227\n",
      "310 / 317 97.79179810725552\n",
      "311 / 318 97.79874213836477\n",
      "312 / 319 97.80564263322884\n",
      "313 / 320 97.8125\n",
      "314 / 321 97.81931464174455\n",
      "315 / 322 97.82608695652173\n",
      "316 / 323 97.83281733746131\n",
      "317 / 324 97.8395061728395\n",
      "318 / 325 97.84615384615384\n",
      "319 / 326 97.85276073619632\n",
      "320 / 327 97.85932721712538\n",
      "321 / 328 97.86585365853658\n",
      "322 / 329 97.87234042553192\n",
      "323 / 330 97.87878787878788\n",
      "324 / 331 97.88519637462235\n",
      "325 / 332 97.89156626506023\n",
      "326 / 333 97.89789789789789\n",
      "327 / 334 97.90419161676647\n",
      "328 / 335 97.91044776119404\n",
      "329 / 336 97.91666666666667\n",
      "330 / 337 97.92284866468843\n",
      "331 / 338 97.92899408284023\n",
      "332 / 339 97.93510324483776\n",
      "333 / 340 97.94117647058823\n",
      "334 / 341 97.94721407624634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335 / 342 97.953216374269\n",
      "336 / 343 97.95918367346938\n",
      "337 / 344 97.96511627906976\n",
      "338 / 345 97.97101449275362\n",
      "339 / 346 97.97687861271676\n",
      "340 / 347 97.98270893371757\n",
      "341 / 348 97.98850574712644\n",
      "342 / 349 97.99426934097421\n",
      "343 / 350 98.0\n",
      "344 / 351 98.00569800569801\n",
      "345 / 352 98.01136363636364\n",
      "346 / 353 98.01699716713881\n",
      "347 / 354 98.0225988700565\n",
      "348 / 355 98.02816901408451\n",
      "349 / 356 98.03370786516854\n",
      "350 / 357 98.03921568627452\n",
      "351 / 358 98.04469273743017\n",
      "352 / 359 98.05013927576601\n",
      "353 / 360 98.05555555555556\n",
      "354 / 361 98.06094182825485\n",
      "355 / 362 98.06629834254143\n",
      "356 / 363 98.07162534435261\n",
      "357 / 364 98.07692307692308\n",
      "358 / 365 98.08219178082192\n",
      "359 / 366 98.08743169398907\n",
      "360 / 367 98.09264305177112\n",
      "361 / 368 98.09782608695652\n",
      "362 / 369 98.1029810298103\n",
      "363 / 370 98.10810810810811\n",
      "364 / 371 98.11320754716981\n",
      "365 / 372 98.11827956989248\n",
      "366 / 373 98.12332439678285\n",
      "367 / 374 98.1283422459893\n",
      "368 / 375 98.13333333333334\n",
      "369 / 376 98.13829787234043\n",
      "370 / 377 98.14323607427056\n",
      "371 / 378 98.14814814814815\n",
      "372 / 379 98.15303430079156\n",
      "373 / 380 98.15789473684211\n",
      "374 / 381 98.16272965879266\n",
      "375 / 382 98.16753926701571\n",
      "376 / 383 98.17232375979113\n",
      "377 / 384 98.17708333333333\n",
      "378 / 385 98.18181818181819\n",
      "379 / 386 98.18652849740933\n",
      "380 / 387 98.19121447028424\n",
      "381 / 388 98.19587628865979\n",
      "382 / 389 98.20051413881748\n",
      "383 / 390 98.2051282051282\n",
      "384 / 391 98.20971867007673\n",
      "385 / 392 98.21428571428571\n",
      "386 / 393 98.21882951653944\n",
      "387 / 394 98.22335025380711\n",
      "388 / 395 98.22784810126582\n",
      "389 / 396 98.23232323232324\n",
      "390 / 397 98.2367758186398\n",
      "391 / 398 98.24120603015075\n",
      "392 / 399 98.24561403508773\n",
      "393 / 400 98.25\n",
      "394 / 401 98.25436408977556\n",
      "395 / 402 98.2587064676617\n",
      "396 / 403 98.26302729528535\n",
      "397 / 404 98.26732673267327\n",
      "398 / 405 98.27160493827161\n",
      "399 / 407 98.03439803439804\n",
      "400 / 408 98.03921568627452\n",
      "401 / 409 98.0440097799511\n",
      "402 / 410 98.04878048780488\n",
      "403 / 411 98.05352798053528\n",
      "404 / 413 97.82082324455206\n",
      "405 / 414 97.82608695652173\n",
      "406 / 415 97.83132530120481\n",
      "407 / 416 97.83653846153847\n",
      "408 / 417 97.84172661870504\n",
      "409 / 418 97.8468899521531\n",
      "410 / 419 97.85202863961814\n",
      "411 / 420 97.85714285714286\n",
      "412 / 421 97.86223277909738\n",
      "413 / 422 97.86729857819905\n",
      "414 / 423 97.87234042553192\n",
      "415 / 424 97.87735849056604\n",
      "416 / 425 97.88235294117646\n",
      "417 / 426 97.88732394366197\n",
      "418 / 427 97.89227166276346\n",
      "419 / 428 97.89719626168224\n",
      "420 / 429 97.9020979020979\n",
      "421 / 430 97.90697674418605\n",
      "422 / 431 97.91183294663573\n",
      "423 / 432 97.91666666666667\n",
      "424 / 433 97.92147806004618\n",
      "425 / 434 97.92626728110599\n",
      "426 / 435 97.93103448275862\n",
      "427 / 436 97.93577981651376\n",
      "428 / 437 97.94050343249428\n",
      "429 / 438 97.94520547945206\n",
      "430 / 439 97.9498861047836\n",
      "431 / 440 97.95454545454545\n",
      "432 / 441 97.95918367346938\n",
      "433 / 442 97.96380090497738\n",
      "434 / 443 97.96839729119638\n",
      "435 / 444 97.97297297297297\n",
      "436 / 445 97.97752808988764\n",
      "437 / 446 97.98206278026906\n",
      "438 / 448 97.76785714285714\n",
      "439 / 449 97.7728285077951\n",
      "440 / 450 97.77777777777777\n",
      "441 / 451 97.78270509977827\n",
      "442 / 452 97.78761061946902\n",
      "443 / 453 97.7924944812362\n",
      "444 / 454 97.79735682819383\n",
      "445 / 455 97.8021978021978\n",
      "446 / 456 97.80701754385964\n",
      "447 / 457 97.81181619256017\n",
      "448 / 458 97.81659388646288\n",
      "449 / 459 97.82135076252723\n",
      "450 / 460 97.82608695652173\n",
      "451 / 461 97.83080260303687\n",
      "452 / 462 97.83549783549783\n",
      "453 / 463 97.8401727861771\n",
      "454 / 464 97.84482758620689\n",
      "455 / 465 97.84946236559139\n",
      "456 / 466 97.85407725321889\n",
      "457 / 467 97.85867237687366\n",
      "458 / 468 97.86324786324786\n",
      "459 / 469 97.86780383795309\n",
      "460 / 470 97.87234042553192\n",
      "461 / 471 97.87685774946921\n",
      "462 / 472 97.88135593220339\n",
      "463 / 473 97.88583509513742\n",
      "464 / 474 97.8902953586498\n",
      "465 / 475 97.89473684210526\n",
      "466 / 476 97.89915966386555\n",
      "467 / 477 97.9035639412998\n",
      "468 / 478 97.90794979079497\n",
      "469 / 480 97.70833333333333\n",
      "470 / 481 97.71309771309771\n",
      "471 / 482 97.71784232365145\n",
      "472 / 483 97.72256728778468\n",
      "473 / 484 97.72727272727273\n",
      "474 / 485 97.73195876288659\n",
      "475 / 486 97.73662551440329\n",
      "476 / 487 97.74127310061601\n",
      "477 / 488 97.74590163934427\n",
      "478 / 489 97.75051124744377\n",
      "479 / 490 97.75510204081633\n",
      "480 / 491 97.75967413441956\n",
      "481 / 492 97.76422764227642\n",
      "482 / 493 97.76876267748479\n",
      "483 / 494 97.77327935222672\n",
      "484 / 495 97.77777777777777\n",
      "485 / 496 97.78225806451613\n",
      "486 / 497 97.78672032193158\n",
      "487 / 498 97.79116465863454\n",
      "488 / 499 97.79559118236473\n",
      "489 / 500 97.8\n",
      "490 / 501 97.80439121756487\n",
      "491 / 502 97.80876494023904\n",
      "492 / 503 97.8131212723658\n",
      "493 / 504 97.81746031746032\n",
      "494 / 505 97.82178217821782\n",
      "495 / 506 97.82608695652173\n",
      "496 / 507 97.83037475345168\n",
      "497 / 508 97.83464566929133\n",
      "498 / 509 97.83889980353635\n",
      "499 / 510 97.84313725490196\n",
      "500 / 511 97.84735812133073\n",
      "501 / 512 97.8515625\n",
      "502 / 513 97.85575048732943\n",
      "503 / 514 97.85992217898833\n",
      "504 / 515 97.86407766990291\n",
      "505 / 516 97.86821705426357\n",
      "506 / 517 97.87234042553192\n",
      "507 / 519 97.6878612716763\n",
      "508 / 520 97.6923076923077\n",
      "509 / 521 97.69673704414588\n",
      "510 / 522 97.70114942528735\n",
      "511 / 523 97.7055449330784\n",
      "512 / 524 97.70992366412214\n",
      "513 / 525 97.71428571428571\n",
      "514 / 526 97.71863117870723\n",
      "515 / 527 97.72296015180265\n",
      "516 / 528 97.72727272727273\n",
      "517 / 530 97.54716981132076\n",
      "518 / 531 97.5517890772128\n",
      "519 / 532 97.55639097744361\n",
      "520 / 533 97.5609756097561\n",
      "521 / 534 97.56554307116104\n",
      "522 / 535 97.57009345794393\n",
      "523 / 536 97.57462686567165\n",
      "524 / 537 97.57914338919926\n",
      "525 / 538 97.58364312267658\n",
      "526 / 539 97.58812615955473\n",
      "527 / 540 97.5925925925926\n",
      "528 / 541 97.59704251386322\n",
      "529 / 542 97.60147601476015\n",
      "530 / 544 97.42647058823529\n",
      "531 / 545 97.43119266055047\n",
      "532 / 546 97.43589743589743\n",
      "533 / 547 97.44058500914076\n",
      "534 / 548 97.44525547445255\n",
      "535 / 549 97.44990892531877\n",
      "536 / 550 97.45454545454545\n",
      "537 / 551 97.45916515426497\n",
      "538 / 552 97.46376811594203\n",
      "539 / 553 97.46835443037975\n",
      "540 / 554 97.47292418772564\n",
      "541 / 555 97.47747747747748\n",
      "542 / 556 97.4820143884892\n",
      "543 / 557 97.48653500897666\n",
      "544 / 558 97.4910394265233\n",
      "545 / 559 97.49552772808586\n",
      "546 / 560 97.5\n",
      "547 / 561 97.50445632798574\n",
      "548 / 562 97.50889679715303\n",
      "549 / 563 97.51332149200711\n",
      "550 / 564 97.51773049645391\n",
      "551 / 565 97.52212389380531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_prediction \u001b[38;5;241m!=\u001b[39m label:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m adv_image \u001b[38;5;241m=\u001b[39m \u001b[43mjsma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m])\n\u001b[0;32m     17\u001b[0m prediction \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model(adv_image))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Correct if the prediction is the target label\u001b[39;00m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mjsma\u001b[1;34m(image, label, step_size, max_iters, model)\u001b[0m\n\u001b[0;32m     22\u001b[0m iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (iter_ \u001b[38;5;241m<\u001b[39m max_iters) \u001b[38;5;129;01mand\u001b[39;00m (prediction \u001b[38;5;241m==\u001b[39m label) \u001b[38;5;129;01mand\u001b[39;00m (search_domain\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Calculate Jacobian matrix \u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m     jacobian \u001b[38;5;241m=\u001b[39m \u001b[43mjsma_jacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Get the two most salient pixels\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     p1, p2 \u001b[38;5;241m=\u001b[39m saliency_map(jacobian, label, increasing, search_domain)\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mjsma_jacobian\u001b[1;34m(model, X)\u001b[0m\n\u001b[0;32m      2\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m image: model(image)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# output shape 10 x 784\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjacobian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:670\u001b[0m, in \u001b[0;36mjacobian\u001b[1;34m(func, inputs, create_graph, strict, vectorize, strategy)\u001b[0m\n\u001b[0;32m    668\u001b[0m jac_i: Tuple[List[torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs)))  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out\u001b[38;5;241m.\u001b[39mnelement()):\n\u001b[1;32m--> 670\u001b[0m     vj \u001b[38;5;241m=\u001b[39m \u001b[43m_autograd_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el_idx, (jac_i_el, vj_el, inp_el) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(jac_i, vj, inputs)):\n\u001b[0;32m    674\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vj_el \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\functional.py:159\u001b[0m, in \u001b[0;36m_autograd_grad\u001b[1;34m(outputs, inputs, grad_outputs, create_graph, retain_graph, is_grads_batched)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_grad_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:276\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "for j in range(1000):\n",
    "    image, label = next(test_loader_iter)\n",
    "    \n",
    "    initial_prediction = torch.argmax(model(image.to(device))).item()\n",
    "    # Don't bother attacking if the image is already misclassified\n",
    "    if initial_prediction != label:\n",
    "        continue\n",
    "\n",
    "    adv_image = jsma(image     = image.to(device), \n",
    "                     label     = label,  \n",
    "                     step_size = 1, \n",
    "                     max_iters = 40,\n",
    "                     model     = model).reshape([1,1,28,28])\n",
    "\n",
    "    prediction = torch.argmax(model(adv_image)).item()\n",
    "\n",
    "    # Correct if the prediction is the target label\n",
    "    if prediction != label:\n",
    "        correct += 1\n",
    "    \n",
    "    print(correct, '/', j+1, correct * 100 / (j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f898382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
